{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'style-based-gan-pytorch'...\n",
      "remote: Enumerating objects: 189, done.\u001b[K\n",
      "remote: Total 189 (delta 0), reused 0 (delta 0), pack-reused 189\u001b[K\n",
      "Receiving objects: 100% (189/189), 34.80 MiB | 21.00 MiB/s, done.\n",
      "Resolving deltas: 100% (94/94), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rosinality/style-based-gan-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  \u001b[01;34mdoc\u001b[0m/         LICENSE   prepare_data.py  \u001b[01;34msample\u001b[0m/\r\n",
      "dataset.py   generate.py  model.py  README.md        train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls style-based-gan-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-11 02:19:01--  https://www.dropbox.com/s/w8xqa8lgkzmhvl4/stylegan-1024px-new.model?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6026:1::a27d:4601, 162.125.70.1\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6026:1::a27d:4601|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/w8xqa8lgkzmhvl4/stylegan-1024px-new.model [following]\n",
      "--2020-05-11 02:19:01--  https://www.dropbox.com/s/dl/w8xqa8lgkzmhvl4/stylegan-1024px-new.model\n",
      "Reusing existing connection to [www.dropbox.com]:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com/cd/0/get/A3c6dBYcOFrMENgv3K66Yse3rzz1rvbhaf_hAN5MiXFA7_U44lvouvggfRYkoc2egwG58TOVR9JDEnabi27Qs1_rElTelbjSNyiT1K_7sMQsM2aYZkdV-PkhB72bP6cw0hc/file?dl=1# [following]\n",
      "--2020-05-11 02:19:02--  https://ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com/cd/0/get/A3c6dBYcOFrMENgv3K66Yse3rzz1rvbhaf_hAN5MiXFA7_U44lvouvggfRYkoc2egwG58TOVR9JDEnabi27Qs1_rElTelbjSNyiT1K_7sMQsM2aYZkdV-PkhB72bP6cw0hc/file?dl=1\n",
      "Resolving ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com (ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com)... 2620:100:6026:6::a27d:4606, 162.125.70.6\n",
      "Connecting to ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com (ucf7839019a686721c5b44c2ae50.dl.dropboxusercontent.com)|2620:100:6026:6::a27d:4606|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 725461173 (692M) [application/binary]\n",
      "Saving to: ‘style-based-gan-pytorch/checkpoint/stylegan-1024px-new.model’\n",
      "\n",
      "style-based-gan-pyt 100%[===================>] 691.85M  41.3MB/s    in 15s     \n",
      "\n",
      "2020-05-11 02:19:17 (45.9 MB/s) - ‘style-based-gan-pytorch/checkpoint/stylegan-1024px-new.model’ saved [725461173/725461173]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.dropbox.com/s/w8xqa8lgkzmhvl4/stylegan-1024px-new.model?dl=1\" -O style-based-gan-pytorch/checkpoint/stylegan-1024px-new.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jruziev/gan/style-based-gan-pytorch\n"
     ]
    }
   ],
   "source": [
    "cd style-based-gan-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py checkpoint/stylegan-1024px-new.model --size 1024 --n_row 2 --n_col 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import utils\n",
    "\n",
    "from model import StyledGenerator\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "generator = StyledGenerator(512).to(device)\n",
    "generator.load_state_dict(torch.load(\"checkpoint/stylegan-1024px-new.model\")['g_running'])\n",
    "generator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_mean_style(generator, device):\n",
    "    mean_style = None\n",
    "\n",
    "    for i in range(10):\n",
    "        style = generator.mean_style(torch.randn(1024, 512).to(device))\n",
    "\n",
    "        if mean_style is None:\n",
    "            mean_style = style\n",
    "\n",
    "        else:\n",
    "            mean_style += style\n",
    "\n",
    "    mean_style /= 10\n",
    "    return mean_style\n",
    "mean_style = get_mean_style(generator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = torch.randn(512, device=device)\n",
    "\n",
    "img1 = generator(point1.view(1,-1), \n",
    "                 step=8, \n",
    "                 mean_style=get_mean_style(generator, device),\n",
    "                 alpha=1,\n",
    "                 style_weight=0.5)\n",
    "utils.save_image(\n",
    "    img1, f'interpol1.png', nrow=1, normalize=True, range=(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "point2 = torch.randn(512, device=device)\n",
    "\n",
    "img2 = generator(point2.view(1,-1), \n",
    "                 step=8, \n",
    "                 mean_style=get_mean_style(generator, device),\n",
    "                 alpha=1,\n",
    "                 style_weight=0.5)\n",
    "utils.save_image(\n",
    "    img2, f'interpol2.png', nrow=1, normalize=True, range=(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    points = [point1.to(device) * w + point2.to(device) * (1 - w) for w in torch.linspace(0, 1, 12, device=device)]\n",
    "    imgs = [generator(point.view(1,-1), \n",
    "                     step=8, \n",
    "                     mean_style=mean_style.to(device),\n",
    "                     alpha=1,\n",
    "                     style_weight=0.5) for point in points]\n",
    "    utils.save_image(\n",
    "        torch.cat(imgs, 0), f'interpol.png', nrow=4, normalize=True, range=(-1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a228 = 228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
